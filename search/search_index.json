{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview This site is just a general site for me to organize my research. Checkout the sidebar. Made by Jasmin Lin.","title":"Home"},{"location":"#overview","text":"This site is just a general site for me to organize my research. Checkout the sidebar. Made by Jasmin Lin.","title":"Overview"},{"location":"embodied/","text":"Embodied Artificial Intelligence Embodied Artificial Intelligence (Embodied AI) refers to the implementation of artificial intelligence into physical robots that interact with the real world . Key Concepts Embodied AI combines computer vision and machine learning . Training process: Pre-training: on large datasets (web, real-world, synthetic data) Post-training: synthetic data is used to test and refine the models Deployment: the model is applied in real environments using computer vision, vision-language models (VLMs), and large language models (LLMs) Real-time interaction: Agents take in contextual information from their surroundings. Process information, plan actions, and execute them using actuators . Behaviors respond to continuous environmental stimuli , adapting future inputs and actions for learning. The models are \u201cintelligent\u201d\u2014able to reason and adapt . References NVIDIA: Embodied AI Glossary ArXiv Paper IEEE Paper Made by Jasmin Lin.","title":"Embodied AI"},{"location":"embodied/#embodied-artificial-intelligence","text":"Embodied Artificial Intelligence (Embodied AI) refers to the implementation of artificial intelligence into physical robots that interact with the real world .","title":"Embodied Artificial Intelligence"},{"location":"embodied/#key-concepts","text":"Embodied AI combines computer vision and machine learning . Training process: Pre-training: on large datasets (web, real-world, synthetic data) Post-training: synthetic data is used to test and refine the models Deployment: the model is applied in real environments using computer vision, vision-language models (VLMs), and large language models (LLMs) Real-time interaction: Agents take in contextual information from their surroundings. Process information, plan actions, and execute them using actuators . Behaviors respond to continuous environmental stimuli , adapting future inputs and actions for learning. The models are \u201cintelligent\u201d\u2014able to reason and adapt .","title":"Key Concepts"},{"location":"embodied/#references","text":"NVIDIA: Embodied AI Glossary ArXiv Paper IEEE Paper Made by Jasmin Lin.","title":"References"},{"location":"foundation/","text":"Foundation Models in Robotics Implementation of foundation models requires pre-training on datasets to allow for higher generalization and adaptability: Robots learn value functions to determine lowest cost, optimal movements High-level understanding of language-driven tasks Zero-shot deployment from simulation-based training Cross-embodied transfer of capabilities Current Challenges Scarce access to data Not all models are able to perform in real-time A model that can generalize across all tasks and environments does not exist Robots lack the ability to truly combine all \u201csenses\u201d to create the perfect behavior Made by Jasmin Lin.","title":"Foundation Models"},{"location":"foundation/#foundation-models-in-robotics","text":"Implementation of foundation models requires pre-training on datasets to allow for higher generalization and adaptability: Robots learn value functions to determine lowest cost, optimal movements High-level understanding of language-driven tasks Zero-shot deployment from simulation-based training Cross-embodied transfer of capabilities","title":"Foundation Models in Robotics"},{"location":"foundation/#current-challenges","text":"Scarce access to data Not all models are able to perform in real-time A model that can generalize across all tasks and environments does not exist Robots lack the ability to truly combine all \u201csenses\u201d to create the perfect behavior Made by Jasmin Lin.","title":"Current Challenges"},{"location":"imitation/","text":"Imitation Learning in Robotics Imitation learning is a paradigm where the model learns without external rewards , relying solely on demonstrations (state-action pairs). Key Concepts Reinforcement learning relies heavily on the reward-update loop, whereas imitation learning does not. Data sources for imitation learning can include: Human demonstrations Teleoperations Synthetic trajectories Videos or motion capture Expert pre-trained policies Methods Behavioral Cloning The agent mimics human demonstrations by copying them. Process: State-action pairs are recorded from expert demonstrations (teleoperation, direct human demos, or recordings). The expert\u2019s output actions become the target outputs for the agent. The model maps input states to actions. Trained models can generalize across different environments. Inverse Reinforcement Learning (IRL) The agent infers the rewards the expert is optimizing and chooses actions based on these inferred rewards. Apprenticeship Learning The agent replicates the goals demonstrated by the expert without using inferred rewards . Benefits Ideal for robots with high degrees of freedom (DoF) . Mimicking human movements results in more fluid robot actions . Robot movements can be improved through continuous practice . Limitations Requires large and varied datasets for proper generalization. Real-world data may contain noise, human error, or inconsistencies that the agent could learn. Small mistakes in imitation may accumulate over time , leading to incorrect learned behaviors. References NVIDIA: Embodied AI Glossary Made by Jasmin Lin.","title":"Imitation Learning"},{"location":"imitation/#imitation-learning-in-robotics","text":"Imitation learning is a paradigm where the model learns without external rewards , relying solely on demonstrations (state-action pairs).","title":"Imitation Learning in Robotics"},{"location":"imitation/#key-concepts","text":"Reinforcement learning relies heavily on the reward-update loop, whereas imitation learning does not. Data sources for imitation learning can include: Human demonstrations Teleoperations Synthetic trajectories Videos or motion capture Expert pre-trained policies","title":"Key Concepts"},{"location":"imitation/#methods","text":"","title":"Methods"},{"location":"imitation/#behavioral-cloning","text":"The agent mimics human demonstrations by copying them. Process: State-action pairs are recorded from expert demonstrations (teleoperation, direct human demos, or recordings). The expert\u2019s output actions become the target outputs for the agent. The model maps input states to actions. Trained models can generalize across different environments.","title":"Behavioral Cloning"},{"location":"imitation/#inverse-reinforcement-learning-irl","text":"The agent infers the rewards the expert is optimizing and chooses actions based on these inferred rewards.","title":"Inverse Reinforcement Learning (IRL)"},{"location":"imitation/#apprenticeship-learning","text":"The agent replicates the goals demonstrated by the expert without using inferred rewards .","title":"Apprenticeship Learning"},{"location":"imitation/#benefits","text":"Ideal for robots with high degrees of freedom (DoF) . Mimicking human movements results in more fluid robot actions . Robot movements can be improved through continuous practice .","title":"Benefits"},{"location":"imitation/#limitations","text":"Requires large and varied datasets for proper generalization. Real-world data may contain noise, human error, or inconsistencies that the agent could learn. Small mistakes in imitation may accumulate over time , leading to incorrect learned behaviors.","title":"Limitations"},{"location":"imitation/#references","text":"NVIDIA: Embodied AI Glossary Made by Jasmin Lin.","title":"References"},{"location":"sensors/","text":"Sensors, Actuators, and Control in Robotics Sensors Sensors are used to assist in environmental perception and navigation: Cameras (visual) Monocular cameras : for 2D image capture Stereo cameras : for 3D image capture, with good depth perception RGB-D cameras : use RGB and depth perception for advanced navigation and environmental interaction Thermal cameras : detect infrared radiation, good for dark or foggy environments Event-based cameras : specialized in high-speed motion detection LIDAR (distance) 2D Lidar : ideal for basic planar navigation 3D Lidar : ideal for wide-field navigation Solid-state, Flash, MEMS Lidar : adds lasers to measure distance, great for mapping Other Sensors IMU : orientation, acceleration GPS : navigation, outdoor localization Radar : distance, speed, size Ultrasonic : distance Wheel encoders : wheel rotation Touch sensors : detect contact Actuators Actuators are used for movement and manipulation based on sensor feedback: Motors : general movement Servos : precision movement Pneumatic/hydraulic actuators : force application Controllers Controllers take in all the input data and output movement: Microcontrollers, single-board controllers, embedded systems Assess the environment using sensors and then use actuators to move Proportional-Integral-Derivative (PID) control : continuously adjusts movements to make real-time corrections Model Predictive Control (MPC) : precisely tracks trajectories by optimizing the robot\u2019s path via predictions Perception These robots have perception\u2014they can combine sensing with visual input to visualize their environment in 3D: Builds 3D map of the environment Tracks its own movement through the environment Constant localization as it moves Made by Jasmin Lin.","title":"Sensors, Actuators, and Control in Robotics"},{"location":"sensors/#sensors-actuators-and-control-in-robotics","text":"","title":"Sensors, Actuators, and Control in Robotics"},{"location":"sensors/#sensors","text":"Sensors are used to assist in environmental perception and navigation:","title":"Sensors"},{"location":"sensors/#cameras-visual","text":"Monocular cameras : for 2D image capture Stereo cameras : for 3D image capture, with good depth perception RGB-D cameras : use RGB and depth perception for advanced navigation and environmental interaction Thermal cameras : detect infrared radiation, good for dark or foggy environments Event-based cameras : specialized in high-speed motion detection","title":"Cameras (visual)"},{"location":"sensors/#lidar-distance","text":"2D Lidar : ideal for basic planar navigation 3D Lidar : ideal for wide-field navigation Solid-state, Flash, MEMS Lidar : adds lasers to measure distance, great for mapping","title":"LIDAR (distance)"},{"location":"sensors/#other-sensors","text":"IMU : orientation, acceleration GPS : navigation, outdoor localization Radar : distance, speed, size Ultrasonic : distance Wheel encoders : wheel rotation Touch sensors : detect contact","title":"Other Sensors"},{"location":"sensors/#actuators","text":"Actuators are used for movement and manipulation based on sensor feedback: Motors : general movement Servos : precision movement Pneumatic/hydraulic actuators : force application","title":"Actuators"},{"location":"sensors/#controllers","text":"Controllers take in all the input data and output movement: Microcontrollers, single-board controllers, embedded systems Assess the environment using sensors and then use actuators to move Proportional-Integral-Derivative (PID) control : continuously adjusts movements to make real-time corrections Model Predictive Control (MPC) : precisely tracks trajectories by optimizing the robot\u2019s path via predictions","title":"Controllers"},{"location":"sensors/#perception","text":"These robots have perception\u2014they can combine sensing with visual input to visualize their environment in 3D: Builds 3D map of the environment Tracks its own movement through the environment Constant localization as it moves Made by Jasmin Lin.","title":"Perception"},{"location":"trossen/","text":"Trossen MobileAI & Robot Operating System (ROS) Training & Simulation Data is collected via teleoperation and imitation learning . LeRobot is used to collect episodes from physical runs or simulation. LeRobot Details Tutorial Video Each arm has its own IP address to connect to the computer. Arms have predefined ranges of motion . Default camera is Intel RealSense , but OpenCV cameras can also be used. Match the camera to the correct index and update the configuration. Note: RealSense cameras may encounter USB bandwidth issues . You may record and upload datasets to platforms like HuggingFace. Episodes can be watched remotely or locally (follower arm mimics your movement). Training & Evaluation The robot tries to mimic the training data you recorded. Checkpoints are automatically saved. Simulation Details (without hardware) MuJoCo window displays the simulation. End-effector control : moves the robot based on scripted end-effector movements. Joint-level control : moves the robot based on controllers. Two-stage pipeline : Runs a scripted policy in the environment Joint trajectories are replayed You can define the environment , data logs , and rewards yourself. Motion logic includes trajectories that define: Time 3D position Orientation Gripper value Interbotix drivers interface between high-level commands and low-level actions. State observations are made using Intel RealSense RGB-D cameras (image and depth). Configuration includes serial numbers and IP addresses. Actions are output by a high-level policy: In MuJoCo: End-effector control : control the gripper pose Joint-level control : control each joint individually The driver (Interbotix) translates commands into motor actions, compensates for gravity, ensures safety, and manages low-level protocols. Robot Operating System (ROS) Repository: Interbotix ROS Core Interbotix Research Robotics Open Standard (IRROS) Layers Hardware Layer Defines actuators and sensors in the robot Includes actuator interface, controllers, drivers, and motors Driver Layer Contains ROS wrappers to obtain data from sensors and actuators Repository resides in this layer Control Layer ROS packages for all Interbotix robot configurations Define parameters for actuators & sensors and manage launch files Application Support Layer Makes working with the robot easier Research Layer User-supplied custom code ROS Functionality Links hardware with software using nodes . Each node can serve a purpose: data collection, reasoning, output. ROS drivers and wrappers allow working with high-level details while covering actuator specifics. ROS Versions ROS 1 (Noetic) Uses a master node to manage all other nodes Good for research; can run on multiple robots with custom networking or ROS namespaces Many supporting resources, but considered legacy ROS 2 (Galactic, Humble, Rolling) Uses DDS (Data Distribution Service) ; no central master node Suitable for real-time robotics and multi-robot communication More modern and actively updated Made by Jasmin Lin.","title":"Trossen MobileAI"},{"location":"trossen/#trossen-mobileai-robot-operating-system-ros","text":"","title":"Trossen MobileAI &amp; Robot Operating System (ROS)"},{"location":"trossen/#training-simulation","text":"Data is collected via teleoperation and imitation learning . LeRobot is used to collect episodes from physical runs or simulation.","title":"Training &amp; Simulation"},{"location":"trossen/#lerobot-details","text":"Tutorial Video Each arm has its own IP address to connect to the computer. Arms have predefined ranges of motion . Default camera is Intel RealSense , but OpenCV cameras can also be used. Match the camera to the correct index and update the configuration. Note: RealSense cameras may encounter USB bandwidth issues . You may record and upload datasets to platforms like HuggingFace. Episodes can be watched remotely or locally (follower arm mimics your movement).","title":"LeRobot Details"},{"location":"trossen/#training-evaluation","text":"The robot tries to mimic the training data you recorded. Checkpoints are automatically saved.","title":"Training &amp; Evaluation"},{"location":"trossen/#simulation-details-without-hardware","text":"MuJoCo window displays the simulation. End-effector control : moves the robot based on scripted end-effector movements. Joint-level control : moves the robot based on controllers. Two-stage pipeline : Runs a scripted policy in the environment Joint trajectories are replayed You can define the environment , data logs , and rewards yourself. Motion logic includes trajectories that define: Time 3D position Orientation Gripper value Interbotix drivers interface between high-level commands and low-level actions. State observations are made using Intel RealSense RGB-D cameras (image and depth). Configuration includes serial numbers and IP addresses. Actions are output by a high-level policy: In MuJoCo: End-effector control : control the gripper pose Joint-level control : control each joint individually The driver (Interbotix) translates commands into motor actions, compensates for gravity, ensures safety, and manages low-level protocols.","title":"Simulation Details (without hardware)"},{"location":"trossen/#robot-operating-system-ros","text":"Repository: Interbotix ROS Core","title":"Robot Operating System (ROS)"},{"location":"trossen/#interbotix-research-robotics-open-standard-irros-layers","text":"Hardware Layer Defines actuators and sensors in the robot Includes actuator interface, controllers, drivers, and motors Driver Layer Contains ROS wrappers to obtain data from sensors and actuators Repository resides in this layer Control Layer ROS packages for all Interbotix robot configurations Define parameters for actuators & sensors and manage launch files Application Support Layer Makes working with the robot easier Research Layer User-supplied custom code","title":"Interbotix Research Robotics Open Standard (IRROS) Layers"},{"location":"trossen/#ros-functionality","text":"Links hardware with software using nodes . Each node can serve a purpose: data collection, reasoning, output. ROS drivers and wrappers allow working with high-level details while covering actuator specifics.","title":"ROS Functionality"},{"location":"trossen/#ros-versions","text":"","title":"ROS Versions"},{"location":"trossen/#ros-1-noetic","text":"Uses a master node to manage all other nodes Good for research; can run on multiple robots with custom networking or ROS namespaces Many supporting resources, but considered legacy","title":"ROS 1 (Noetic)"},{"location":"trossen/#ros-2-galactic-humble-rolling","text":"Uses DDS (Data Distribution Service) ; no central master node Suitable for real-time robotics and multi-robot communication More modern and actively updated Made by Jasmin Lin.","title":"ROS 2 (Galactic, Humble, Rolling)"}]}