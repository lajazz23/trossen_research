{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview This site is just a general site for me to organize my research. Checkout the sidebar. Made by Jasmin Lin.","title":"Home"},{"location":"#overview","text":"This site is just a general site for me to organize my research. Checkout the sidebar. Made by Jasmin Lin.","title":"Overview"},{"location":"claw/","text":"CLAW (CLIP-Language-Action for Weight) CLAW is a weight-aware Vision-Language-Action (VLA) model that decouples condition evaluation from action generation. Key Features Uses a condition-monitoring module to evaluate numeric task constraints (e.g., weight thresholds) for action output. Combines CLIP and \u03c00 for continuous visuomotor control: CLIP converts numeric readings into symbolic language prompts, ideal for detecting when a weight threshold is reached. - Determines if the robot should grip harder or stop. \u03c00 produces continuous, high-frequency robot actions from multi-view observations . - Maps observations + language input into continuous motor commands. Together: - CLIP labels the task (\"grip\" or \"no grip\") - Passes this prompt to \u03c00 - \u03c00 is fine-tuned using flow-matching loss conditioned on the prompts Motivation Standard VLAs often struggle with precise execution , particularly for manipulation tasks like picking up objects . CLAW addresses this by integrating weight awareness into the decision-making process. CLAW Pipeline Language Input CLIP monitors the scale \u2192 outputs \u201cstop\u201d or \u201ccontinue\u201d based on weight threshold \u03c00 generates continuous robot actions using the CLIP prompt + multi-view images Loop continues until the target weight is reached Made by Jasmin Lin.","title":"CLAW"},{"location":"claw/#claw-clip-language-action-for-weight","text":"CLAW is a weight-aware Vision-Language-Action (VLA) model that decouples condition evaluation from action generation.","title":"CLAW (CLIP-Language-Action for Weight)"},{"location":"claw/#key-features","text":"Uses a condition-monitoring module to evaluate numeric task constraints (e.g., weight thresholds) for action output. Combines CLIP and \u03c00 for continuous visuomotor control: CLIP converts numeric readings into symbolic language prompts, ideal for detecting when a weight threshold is reached. - Determines if the robot should grip harder or stop. \u03c00 produces continuous, high-frequency robot actions from multi-view observations . - Maps observations + language input into continuous motor commands. Together: - CLIP labels the task (\"grip\" or \"no grip\") - Passes this prompt to \u03c00 - \u03c00 is fine-tuned using flow-matching loss conditioned on the prompts","title":"Key Features"},{"location":"claw/#motivation","text":"Standard VLAs often struggle with precise execution , particularly for manipulation tasks like picking up objects . CLAW addresses this by integrating weight awareness into the decision-making process.","title":"Motivation"},{"location":"claw/#claw-pipeline","text":"Language Input CLIP monitors the scale \u2192 outputs \u201cstop\u201d or \u201ccontinue\u201d based on weight threshold \u03c00 generates continuous robot actions using the CLIP prompt + multi-view images Loop continues until the target weight is reached Made by Jasmin Lin.","title":"CLAW Pipeline"},{"location":"dualarm/","text":"Dual-arm Dextrous Manipulation Made by Jasmin Lin.","title":"Dual-arm Dextrous Manipulation"},{"location":"dualarm/#dual-arm-dextrous-manipulation","text":"Made by Jasmin Lin.","title":"Dual-arm Dextrous Manipulation"},{"location":"embodied/","text":"Embodied Artificial Intelligence Embodied Artificial Intelligence (Embodied AI) refers to the implementation of artificial intelligence into physical robots that interact with the real world . Key Concepts Embodied AI combines computer vision and machine learning . Training process: Pre-training: on large datasets (web, real-world, synthetic data) Post-training: synthetic data is used to test and refine the models Deployment: the model is applied in real environments using computer vision, vision-language models (VLMs), and large language models (LLMs) Real-time interaction: Agents take in contextual information from their surroundings. Process information, plan actions, and execute them using actuators . Behaviors respond to continuous environmental stimuli , adapting future inputs and actions for learning. The models are \u201cintelligent\u201d\u2014able to reason and adapt . References NVIDIA: Embodied AI Glossary ArXiv Paper IEEE Paper Made by Jasmin Lin.","title":"Embodied AI"},{"location":"embodied/#embodied-artificial-intelligence","text":"Embodied Artificial Intelligence (Embodied AI) refers to the implementation of artificial intelligence into physical robots that interact with the real world .","title":"Embodied Artificial Intelligence"},{"location":"embodied/#key-concepts","text":"Embodied AI combines computer vision and machine learning . Training process: Pre-training: on large datasets (web, real-world, synthetic data) Post-training: synthetic data is used to test and refine the models Deployment: the model is applied in real environments using computer vision, vision-language models (VLMs), and large language models (LLMs) Real-time interaction: Agents take in contextual information from their surroundings. Process information, plan actions, and execute them using actuators . Behaviors respond to continuous environmental stimuli , adapting future inputs and actions for learning. The models are \u201cintelligent\u201d\u2014able to reason and adapt .","title":"Key Concepts"},{"location":"embodied/#references","text":"NVIDIA: Embodied AI Glossary ArXiv Paper IEEE Paper Made by Jasmin Lin.","title":"References"},{"location":"foundation/","text":"Foundation Models in Robotics Implementation of foundation models requires pre-training on datasets to allow for higher generalization and adaptability: Robots learn value functions to determine lowest cost, optimal movements High-level understanding of language-driven tasks Zero-shot deployment from simulation-based training Cross-embodied transfer of capabilities Current Challenges Scarce access to data Not all models are able to perform in real-time A model that can generalize across all tasks and environments does not exist Robots lack the ability to truly combine all \u201csenses\u201d to create the perfect behavior Made by Jasmin Lin.","title":"Foundation Models"},{"location":"foundation/#foundation-models-in-robotics","text":"Implementation of foundation models requires pre-training on datasets to allow for higher generalization and adaptability: Robots learn value functions to determine lowest cost, optimal movements High-level understanding of language-driven tasks Zero-shot deployment from simulation-based training Cross-embodied transfer of capabilities","title":"Foundation Models in Robotics"},{"location":"foundation/#current-challenges","text":"Scarce access to data Not all models are able to perform in real-time A model that can generalize across all tasks and environments does not exist Robots lack the ability to truly combine all \u201csenses\u201d to create the perfect behavior Made by Jasmin Lin.","title":"Current Challenges"},{"location":"groot/","text":"GR00T N1 (NVIDIA) GR00T N1 is a Vision-Language-Action (VLA) model that generates actions from images and language instruction inputs and uses a dual-system compositional architecture . Dual-System Compositional Architecture Based on human cognition, the model has two systems : System 1 \u2013 fast, instinctive, automatic System 2 \u2013 slow, thoughtful, logical System 2 (Vision-Language-Action) Understands words and images together Uses the robot\u2019s camera feed and given instructions to determine actions Operates at 10 Hz , slower but allows for more reasoning System 1 (Diffusion Transformer) Uses System 2\u2019s output and its sensors to decide movement Operates at 120 Hz Uses diffusion to ensure smooth motion Cooperates with System 2 (System 1 listens to System 2) Data Pyramid Structure To prevent data overload, the model organizes data hierarchically: Base: large quantities of web data and human videos Middle: synthetic data Top: real-world data collected by physical robots Co-Training Strategy Trains using action-less data sources with a latent-action codebook and a trained inverse dynamics model (IDM) : Latent-action compresses movement patterns into reusable codes for guessing actions without labels IDM predicts missing frames in videos, outputting estimated actions Robot is trained on a mix of real robot data, synthetic video, and human videos Eliminates the need for large pools of labeled data GR00T N1 Foundation Model A VLA model that can interact with its environment Uses NVIDIA Eagle-2 VLM as backbone: Understands the image Interprets the goal Encodes information into tokens for downstream analysis Uses the Diffusion Transformer : Takes encoded visuals and VLM info Quickly generates movements Performs flow-matching for smooth motion Model Architecture State and Action Encoders normalize representations across robots Small neural net converts actions and raw states into a shared space Action encoder incorporates timesteps Flow matching : uses ground-truth actions and Gaussian noise to train the model to predict denoising vector fields Eagle-2 VLM integrates SmolLM2 and SigLIP-2 to interpret images + text instructions Images resized and passed through pixel shuffle Combined with text in a chat-style format Diffusion Transformer predicts actions from inputs Alternates between processing actions and injecting vision-language features Action decoder outputs clean predicted actions Designed for multi-robot compatibility , accommodating different action spaces Generation of Training Data Organized according to the data pyramid High-quality data comes from real humans controlling robots (teleoperation) VQ-VAE generates latent actions from videos lacking robot action labels Improves efficiency and lowers cost of real robot data Neural Trajectories Video generation models synthesize robot behavior based on existing data (~10\u00d7 more data) Multimodal LLM improves quality/diversity by detecting objects and generating task variations LLM assesses whether generated actions match the intended prompt Simulation Trajectories DexMimicGen generates large-scale simulated robot data: Breaks data into action chunks Modifies small details (object location) while preserving action Smooth transitions between frames Saves data only if the robot succeeds in simulation Training Details Pre-training: flow-matching on real and synthetic robot + human data Extracts latent actions from human videos without ground-truth actions Post-training: fine-tunes the pretrained model on embodiment-specific datasets Neural trajectories generate robotic behaviors for missing real-world teleoperation data Generates visuals only Uses latent action codes IDM estimates actions from video changes DexMimicGen generates high-quality training datasets Source demonstrations collected via Leap Motion Devices, which DexMimicGen uses to synthesize new demos Evaluation: tabletop tasks like pick-and-place, object passing, manipulation, coordination Made by Jasmin Lin.","title":"GR00T-N1/1.5"},{"location":"groot/#gr00t-n1-nvidia","text":"GR00T N1 is a Vision-Language-Action (VLA) model that generates actions from images and language instruction inputs and uses a dual-system compositional architecture .","title":"GR00T N1 (NVIDIA)"},{"location":"groot/#dual-system-compositional-architecture","text":"Based on human cognition, the model has two systems : System 1 \u2013 fast, instinctive, automatic System 2 \u2013 slow, thoughtful, logical","title":"Dual-System Compositional Architecture"},{"location":"groot/#system-2-vision-language-action","text":"Understands words and images together Uses the robot\u2019s camera feed and given instructions to determine actions Operates at 10 Hz , slower but allows for more reasoning","title":"System 2 (Vision-Language-Action)"},{"location":"groot/#system-1-diffusion-transformer","text":"Uses System 2\u2019s output and its sensors to decide movement Operates at 120 Hz Uses diffusion to ensure smooth motion Cooperates with System 2 (System 1 listens to System 2)","title":"System 1 (Diffusion Transformer)"},{"location":"groot/#data-pyramid-structure","text":"To prevent data overload, the model organizes data hierarchically: Base: large quantities of web data and human videos Middle: synthetic data Top: real-world data collected by physical robots","title":"Data Pyramid Structure"},{"location":"groot/#co-training-strategy","text":"Trains using action-less data sources with a latent-action codebook and a trained inverse dynamics model (IDM) : Latent-action compresses movement patterns into reusable codes for guessing actions without labels IDM predicts missing frames in videos, outputting estimated actions Robot is trained on a mix of real robot data, synthetic video, and human videos Eliminates the need for large pools of labeled data","title":"Co-Training Strategy"},{"location":"groot/#gr00t-n1-foundation-model","text":"A VLA model that can interact with its environment Uses NVIDIA Eagle-2 VLM as backbone: Understands the image Interprets the goal Encodes information into tokens for downstream analysis Uses the Diffusion Transformer : Takes encoded visuals and VLM info Quickly generates movements Performs flow-matching for smooth motion","title":"GR00T N1 Foundation Model"},{"location":"groot/#model-architecture","text":"State and Action Encoders normalize representations across robots Small neural net converts actions and raw states into a shared space Action encoder incorporates timesteps Flow matching : uses ground-truth actions and Gaussian noise to train the model to predict denoising vector fields Eagle-2 VLM integrates SmolLM2 and SigLIP-2 to interpret images + text instructions Images resized and passed through pixel shuffle Combined with text in a chat-style format Diffusion Transformer predicts actions from inputs Alternates between processing actions and injecting vision-language features Action decoder outputs clean predicted actions Designed for multi-robot compatibility , accommodating different action spaces","title":"Model Architecture"},{"location":"groot/#generation-of-training-data","text":"Organized according to the data pyramid High-quality data comes from real humans controlling robots (teleoperation) VQ-VAE generates latent actions from videos lacking robot action labels Improves efficiency and lowers cost of real robot data","title":"Generation of Training Data"},{"location":"groot/#neural-trajectories","text":"Video generation models synthesize robot behavior based on existing data (~10\u00d7 more data) Multimodal LLM improves quality/diversity by detecting objects and generating task variations LLM assesses whether generated actions match the intended prompt","title":"Neural Trajectories"},{"location":"groot/#simulation-trajectories","text":"DexMimicGen generates large-scale simulated robot data: Breaks data into action chunks Modifies small details (object location) while preserving action Smooth transitions between frames Saves data only if the robot succeeds in simulation","title":"Simulation Trajectories"},{"location":"groot/#training-details","text":"Pre-training: flow-matching on real and synthetic robot + human data Extracts latent actions from human videos without ground-truth actions Post-training: fine-tunes the pretrained model on embodiment-specific datasets Neural trajectories generate robotic behaviors for missing real-world teleoperation data Generates visuals only Uses latent action codes IDM estimates actions from video changes DexMimicGen generates high-quality training datasets Source demonstrations collected via Leap Motion Devices, which DexMimicGen uses to synthesize new demos Evaluation: tabletop tasks like pick-and-place, object passing, manipulation, coordination Made by Jasmin Lin.","title":"Training Details"},{"location":"imitation/","text":"Imitation Learning in Robotics Imitation learning is a paradigm where the model learns without external rewards , relying solely on demonstrations (state-action pairs). Key Concepts Reinforcement learning relies heavily on the reward-update loop, whereas imitation learning does not. Data sources for imitation learning can include: Human demonstrations Teleoperations Synthetic trajectories Videos or motion capture Expert pre-trained policies Methods Behavioral Cloning The agent mimics human demonstrations by copying them. Process: State-action pairs are recorded from expert demonstrations (teleoperation, direct human demos, or recordings). The expert\u2019s output actions become the target outputs for the agent. The model maps input states to actions. Trained models can generalize across different environments. Inverse Reinforcement Learning (IRL) The agent infers the rewards the expert is optimizing and chooses actions based on these inferred rewards. Apprenticeship Learning The agent replicates the goals demonstrated by the expert without using inferred rewards . Benefits Ideal for robots with high degrees of freedom (DoF) . Mimicking human movements results in more fluid robot actions . Robot movements can be improved through continuous practice . Limitations Requires large and varied datasets for proper generalization. Real-world data may contain noise, human error, or inconsistencies that the agent could learn. Small mistakes in imitation may accumulate over time , leading to incorrect learned behaviors. References NVIDIA: Embodied AI Glossary Made by Jasmin Lin.","title":"Imitation Learning"},{"location":"imitation/#imitation-learning-in-robotics","text":"Imitation learning is a paradigm where the model learns without external rewards , relying solely on demonstrations (state-action pairs).","title":"Imitation Learning in Robotics"},{"location":"imitation/#key-concepts","text":"Reinforcement learning relies heavily on the reward-update loop, whereas imitation learning does not. Data sources for imitation learning can include: Human demonstrations Teleoperations Synthetic trajectories Videos or motion capture Expert pre-trained policies","title":"Key Concepts"},{"location":"imitation/#methods","text":"","title":"Methods"},{"location":"imitation/#behavioral-cloning","text":"The agent mimics human demonstrations by copying them. Process: State-action pairs are recorded from expert demonstrations (teleoperation, direct human demos, or recordings). The expert\u2019s output actions become the target outputs for the agent. The model maps input states to actions. Trained models can generalize across different environments.","title":"Behavioral Cloning"},{"location":"imitation/#inverse-reinforcement-learning-irl","text":"The agent infers the rewards the expert is optimizing and chooses actions based on these inferred rewards.","title":"Inverse Reinforcement Learning (IRL)"},{"location":"imitation/#apprenticeship-learning","text":"The agent replicates the goals demonstrated by the expert without using inferred rewards .","title":"Apprenticeship Learning"},{"location":"imitation/#benefits","text":"Ideal for robots with high degrees of freedom (DoF) . Mimicking human movements results in more fluid robot actions . Robot movements can be improved through continuous practice .","title":"Benefits"},{"location":"imitation/#limitations","text":"Requires large and varied datasets for proper generalization. Real-world data may contain noise, human error, or inconsistencies that the agent could learn. Small mistakes in imitation may accumulate over time , leading to incorrect learned behaviors.","title":"Limitations"},{"location":"imitation/#references","text":"NVIDIA: Embodied AI Glossary Made by Jasmin Lin.","title":"References"},{"location":"pi/","text":"\u03c00.5 \u03c00.5 is a multimodal vision-language-action (VLA) model used to reason and understand scenes and instructions. Overview Uses co-training on heterogeneous tasks to enable broad generalization Trains on data from multiple robots , high-level semantic predictions, web data, and other sources Capable of dexterous tasks , such as cleaning homes Robots only become useful outside the lab in real-world scenarios Can deduce actions for tasks on their own and transfer knowledge from other systems Streamlines multiple types of data into a single readable datasheet Can perform tasks it has not been explicitly trained on , by dividing them into subtasks automatically Uses a hierarchical architecture with 2 levels: Base model trained on many tasks Fine-tuned for real robot tasks Predicts the next semantic subtask by generating short action tasks Previous Work / Significance Earlier robots were narrowly trained and limited Modern models can perform more tasks in more environments across different robot types VLAs take visual + language input and output clean actions Co-trained on videos with subtitles, robot simulations, text descriptions, and manual demos \u03c00.5 combines: Web-scale vision-language data Simulation & robot demos Semantic labels & verbal instructions Knowledge transferred from other robots Goal: generalization across unseen objects, instructions, environments, and untrained tasks Training and Model Structure Base model: vision-language model that understands scenes and semantics Built on strong visual-language priors Pre-training: enables robot to understand relevant tasks Inputs/outputs are converted into discrete tokens Post-training: specializes the model for mobile manipulation Includes an action expert predicting low-level control actions Outputs detailed motor commands for subtasks Model input: images or robot state Model output: string tokens and motor commands Data blending: converts all inputs into tokens for efficiency Uses discrete tokens + flow-matching simultaneously Flow-matching produces smooth actions Discrete tokens compress data for prediction Training: autoregressive transformer predicts next token in sequence Testing: skips token decoding, uses flow-matching for action execution Multimodal Grounding Model learns tasks may differ across robots/environments Follows a hierarchical plan Multimodal grounding: Language \u2194 Vision: identifies subjects in tasks (e.g., cup location, floor texture, color) Vision \u2194 Action: maps object location to controlled actions Language \u2194 Action: interprets text instructions into trajectories Vision + Language \u2194 Action: generates action tokens/flow vectors from image + text Uses images to perceive surroundings, language to determine intent, and decides actions Learns relationships between text, images, and motion Action Expert Predicts continuous actions using flow-matching Outputs real-valued control vectors , e.g., joint angles, base velocities Hardware Control System Experiment run on two mobile manipulator platforms \u03c00.5 outputs target poses for: Each arm joint Gripper position Torso lift Base Predicts multi-step action chunks instead of step-by-step actions Can produce plans for different robot configurations Fuses multi-view visual data across tasks Experimental Evaluation Robots successfully accomplished tasks in mock and real environments More diverse training environments \u2192 better generalization Full pre-training is necessary for optimal results Mixed data: web, robot, subtask annotations High training exposure improves language following / prompt understanding High-level interference: robot determines the next task Domain-specific robot data critical for planning: Object names and relationships Human instructions Subtask breakdown Limitations Unfamiliar items in environments remain challenging Partial observability affects robot function (e.g., arm blocking view) High-level subtask interference can cause repeated actions or distractions Can only process simple prompts (e.g., \"clean this up\") Made by Jasmin Lin.","title":"Pi0.5"},{"location":"pi/#05","text":"\u03c00.5 is a multimodal vision-language-action (VLA) model used to reason and understand scenes and instructions.","title":"\u03c00.5"},{"location":"pi/#overview","text":"Uses co-training on heterogeneous tasks to enable broad generalization Trains on data from multiple robots , high-level semantic predictions, web data, and other sources Capable of dexterous tasks , such as cleaning homes Robots only become useful outside the lab in real-world scenarios Can deduce actions for tasks on their own and transfer knowledge from other systems Streamlines multiple types of data into a single readable datasheet Can perform tasks it has not been explicitly trained on , by dividing them into subtasks automatically Uses a hierarchical architecture with 2 levels: Base model trained on many tasks Fine-tuned for real robot tasks Predicts the next semantic subtask by generating short action tasks","title":"Overview"},{"location":"pi/#previous-work-significance","text":"Earlier robots were narrowly trained and limited Modern models can perform more tasks in more environments across different robot types VLAs take visual + language input and output clean actions Co-trained on videos with subtitles, robot simulations, text descriptions, and manual demos \u03c00.5 combines: Web-scale vision-language data Simulation & robot demos Semantic labels & verbal instructions Knowledge transferred from other robots Goal: generalization across unseen objects, instructions, environments, and untrained tasks","title":"Previous Work / Significance"},{"location":"pi/#training-and-model-structure","text":"Base model: vision-language model that understands scenes and semantics Built on strong visual-language priors Pre-training: enables robot to understand relevant tasks Inputs/outputs are converted into discrete tokens Post-training: specializes the model for mobile manipulation Includes an action expert predicting low-level control actions Outputs detailed motor commands for subtasks Model input: images or robot state Model output: string tokens and motor commands Data blending: converts all inputs into tokens for efficiency Uses discrete tokens + flow-matching simultaneously Flow-matching produces smooth actions Discrete tokens compress data for prediction Training: autoregressive transformer predicts next token in sequence Testing: skips token decoding, uses flow-matching for action execution","title":"Training and Model Structure"},{"location":"pi/#multimodal-grounding","text":"Model learns tasks may differ across robots/environments Follows a hierarchical plan Multimodal grounding: Language \u2194 Vision: identifies subjects in tasks (e.g., cup location, floor texture, color) Vision \u2194 Action: maps object location to controlled actions Language \u2194 Action: interprets text instructions into trajectories Vision + Language \u2194 Action: generates action tokens/flow vectors from image + text Uses images to perceive surroundings, language to determine intent, and decides actions Learns relationships between text, images, and motion","title":"Multimodal Grounding"},{"location":"pi/#action-expert","text":"Predicts continuous actions using flow-matching Outputs real-valued control vectors , e.g., joint angles, base velocities","title":"Action Expert"},{"location":"pi/#hardware-control-system","text":"Experiment run on two mobile manipulator platforms \u03c00.5 outputs target poses for: Each arm joint Gripper position Torso lift Base Predicts multi-step action chunks instead of step-by-step actions Can produce plans for different robot configurations Fuses multi-view visual data across tasks","title":"Hardware Control System"},{"location":"pi/#experimental-evaluation","text":"Robots successfully accomplished tasks in mock and real environments More diverse training environments \u2192 better generalization Full pre-training is necessary for optimal results Mixed data: web, robot, subtask annotations High training exposure improves language following / prompt understanding High-level interference: robot determines the next task Domain-specific robot data critical for planning: Object names and relationships Human instructions Subtask breakdown","title":"Experimental Evaluation"},{"location":"pi/#limitations","text":"Unfamiliar items in environments remain challenging Partial observability affects robot function (e.g., arm blocking view) High-level subtask interference can cause repeated actions or distractions Can only process simple prompts (e.g., \"clean this up\") Made by Jasmin Lin.","title":"Limitations"},{"location":"rt/","text":"RT-1 (Google) RT-1 is a real-time transformer-based robot policy that can control real-world robots. Overview Designed for everyday manipulation tasks : opening doors, moving objects, etc. Takes language instructions and a short video from the robot camera to guide actions Learns directly from human demonstrations , mimicking actions Capable of generalization, scaled training, and real-world execution Can generalize new tasks without retraining due to language-conditioning and diverse training Direct robot control policy (not a vision-language-action model) Processes a series of images and instructions, not just a single image Core Transformer core adapted for robotics EfficientNet-B3 for image processing FiLM layers integrate language instructions with visual features TokenLearner filters for the most relevant information Transformer outputs decisions about the robot\u2019s actions Method Inputs processed by the core to extract and compact visual tokens Decoder-only transformer processes tokens and outputs discrete tokens for each limb (\u03c00.5 outputs text tokens, which are then passed to a control policy) Trained on 130k real robot demonstrations , not just internet data Runs at 3 Hz , providing fast action response for robot execution Made by Jasmin Lin.","title":"RT-1"},{"location":"rt/#rt-1-google","text":"RT-1 is a real-time transformer-based robot policy that can control real-world robots.","title":"RT-1 (Google)"},{"location":"rt/#overview","text":"Designed for everyday manipulation tasks : opening doors, moving objects, etc. Takes language instructions and a short video from the robot camera to guide actions Learns directly from human demonstrations , mimicking actions Capable of generalization, scaled training, and real-world execution Can generalize new tasks without retraining due to language-conditioning and diverse training Direct robot control policy (not a vision-language-action model) Processes a series of images and instructions, not just a single image","title":"Overview"},{"location":"rt/#core","text":"Transformer core adapted for robotics EfficientNet-B3 for image processing FiLM layers integrate language instructions with visual features TokenLearner filters for the most relevant information Transformer outputs decisions about the robot\u2019s actions","title":"Core"},{"location":"rt/#method","text":"Inputs processed by the core to extract and compact visual tokens Decoder-only transformer processes tokens and outputs discrete tokens for each limb (\u03c00.5 outputs text tokens, which are then passed to a control policy) Trained on 130k real robot demonstrations , not just internet data Runs at 3 Hz , providing fast action response for robot execution Made by Jasmin Lin.","title":"Method"},{"location":"sensors/","text":"Sensors, Actuators, and Control in Robotics Sensors Sensors are used to assist in environmental perception and navigation: Cameras (visual) Monocular cameras : for 2D image capture Stereo cameras : for 3D image capture, with good depth perception RGB-D cameras : use RGB and depth perception for advanced navigation and environmental interaction Thermal cameras : detect infrared radiation, good for dark or foggy environments Event-based cameras : specialized in high-speed motion detection LIDAR (distance) 2D Lidar : ideal for basic planar navigation 3D Lidar : ideal for wide-field navigation Solid-state, Flash, MEMS Lidar : adds lasers to measure distance, great for mapping Other Sensors IMU : orientation, acceleration GPS : navigation, outdoor localization Radar : distance, speed, size Ultrasonic : distance Wheel encoders : wheel rotation Touch sensors : detect contact Actuators Actuators are used for movement and manipulation based on sensor feedback: Motors : general movement Servos : precision movement Pneumatic/hydraulic actuators : force application Controllers Controllers take in all the input data and output movement: Microcontrollers, single-board controllers, embedded systems Assess the environment using sensors and then use actuators to move Proportional-Integral-Derivative (PID) control : continuously adjusts movements to make real-time corrections Model Predictive Control (MPC) : precisely tracks trajectories by optimizing the robot\u2019s path via predictions Perception These robots have perception\u2014they can combine sensing with visual input to visualize their environment in 3D: Builds 3D map of the environment Tracks its own movement through the environment Constant localization as it moves Made by Jasmin Lin.","title":"Sensors, Actuators, and Control in Robotics"},{"location":"sensors/#sensors-actuators-and-control-in-robotics","text":"","title":"Sensors, Actuators, and Control in Robotics"},{"location":"sensors/#sensors","text":"Sensors are used to assist in environmental perception and navigation:","title":"Sensors"},{"location":"sensors/#cameras-visual","text":"Monocular cameras : for 2D image capture Stereo cameras : for 3D image capture, with good depth perception RGB-D cameras : use RGB and depth perception for advanced navigation and environmental interaction Thermal cameras : detect infrared radiation, good for dark or foggy environments Event-based cameras : specialized in high-speed motion detection","title":"Cameras (visual)"},{"location":"sensors/#lidar-distance","text":"2D Lidar : ideal for basic planar navigation 3D Lidar : ideal for wide-field navigation Solid-state, Flash, MEMS Lidar : adds lasers to measure distance, great for mapping","title":"LIDAR (distance)"},{"location":"sensors/#other-sensors","text":"IMU : orientation, acceleration GPS : navigation, outdoor localization Radar : distance, speed, size Ultrasonic : distance Wheel encoders : wheel rotation Touch sensors : detect contact","title":"Other Sensors"},{"location":"sensors/#actuators","text":"Actuators are used for movement and manipulation based on sensor feedback: Motors : general movement Servos : precision movement Pneumatic/hydraulic actuators : force application","title":"Actuators"},{"location":"sensors/#controllers","text":"Controllers take in all the input data and output movement: Microcontrollers, single-board controllers, embedded systems Assess the environment using sensors and then use actuators to move Proportional-Integral-Derivative (PID) control : continuously adjusts movements to make real-time corrections Model Predictive Control (MPC) : precisely tracks trajectories by optimizing the robot\u2019s path via predictions","title":"Controllers"},{"location":"sensors/#perception","text":"These robots have perception\u2014they can combine sensing with visual input to visualize their environment in 3D: Builds 3D map of the environment Tracks its own movement through the environment Constant localization as it moves Made by Jasmin Lin.","title":"Perception"},{"location":"trossen/","text":"Trossen MobileAI & Robot Operating System (ROS) Training & Simulation Data is collected via teleoperation and imitation learning . LeRobot is used to collect episodes from physical runs or simulation. LeRobot Details Tutorial Video Each arm has its own IP address to connect to the computer. Arms have predefined ranges of motion . Default camera is Intel RealSense , but OpenCV cameras can also be used. Match the camera to the correct index and update the configuration. Note: RealSense cameras may encounter USB bandwidth issues . You may record and upload datasets to platforms like HuggingFace. Episodes can be watched remotely or locally (follower arm mimics your movement). Training & Evaluation The robot tries to mimic the training data you recorded. Checkpoints are automatically saved. Simulation Details (without hardware) MuJoCo window displays the simulation. End-effector control : moves the robot based on scripted end-effector movements. Joint-level control : moves the robot based on controllers. Two-stage pipeline : Runs a scripted policy in the environment Joint trajectories are replayed You can define the environment , data logs , and rewards yourself. Motion logic includes trajectories that define: Time 3D position Orientation Gripper value Interbotix drivers interface between high-level commands and low-level actions. State observations are made using Intel RealSense RGB-D cameras (image and depth). Configuration includes serial numbers and IP addresses. Actions are output by a high-level policy: In MuJoCo: End-effector control : control the gripper pose Joint-level control : control each joint individually The driver (Interbotix) translates commands into motor actions, compensates for gravity, ensures safety, and manages low-level protocols. Robot Operating System (ROS) Repository: Interbotix ROS Core Interbotix Research Robotics Open Standard (IRROS) Layers 1. Hardware Layer - Defines actuators and sensors in the robot - Includes actuator interface, controllers, drivers, and motors 2. Driver Layer - Contains ROS wrappers to obtain data from sensors and actuators - Repository resides in this layer 3. Control Layer - ROS packages for all Interbotix robot configurations - Define parameters for actuators & sensors and manage launch files 4. Application Support Layer - Makes working with the robot easier 5. Research Layer - User-supplied custom code ROS Functionality Links hardware with software using nodes . Each node can serve a purpose: data collection, reasoning, output. ROS drivers and wrappers allow working with high-level details while covering actuator specifics. ROS Versions ROS 1 (Noetic) Uses a master node to manage all other nodes Good for research; can run on multiple robots with custom networking or ROS namespaces Many supporting resources, but considered legacy ROS 2 (Galactic, Humble, Rolling) Uses DDS (Data Distribution Service) ; no central master node Suitable for real-time robotics and multi-robot communication More modern and actively updated Made by Jasmin Lin.","title":"Trossen MobileAI"},{"location":"trossen/#trossen-mobileai-robot-operating-system-ros","text":"","title":"Trossen MobileAI &amp; Robot Operating System (ROS)"},{"location":"trossen/#training-simulation","text":"Data is collected via teleoperation and imitation learning . LeRobot is used to collect episodes from physical runs or simulation.","title":"Training &amp; Simulation"},{"location":"trossen/#lerobot-details","text":"Tutorial Video Each arm has its own IP address to connect to the computer. Arms have predefined ranges of motion . Default camera is Intel RealSense , but OpenCV cameras can also be used. Match the camera to the correct index and update the configuration. Note: RealSense cameras may encounter USB bandwidth issues . You may record and upload datasets to platforms like HuggingFace. Episodes can be watched remotely or locally (follower arm mimics your movement).","title":"LeRobot Details"},{"location":"trossen/#training-evaluation","text":"The robot tries to mimic the training data you recorded. Checkpoints are automatically saved.","title":"Training &amp; Evaluation"},{"location":"trossen/#simulation-details-without-hardware","text":"MuJoCo window displays the simulation. End-effector control : moves the robot based on scripted end-effector movements. Joint-level control : moves the robot based on controllers. Two-stage pipeline : Runs a scripted policy in the environment Joint trajectories are replayed You can define the environment , data logs , and rewards yourself. Motion logic includes trajectories that define: Time 3D position Orientation Gripper value Interbotix drivers interface between high-level commands and low-level actions. State observations are made using Intel RealSense RGB-D cameras (image and depth). Configuration includes serial numbers and IP addresses. Actions are output by a high-level policy: In MuJoCo: End-effector control : control the gripper pose Joint-level control : control each joint individually The driver (Interbotix) translates commands into motor actions, compensates for gravity, ensures safety, and manages low-level protocols.","title":"Simulation Details (without hardware)"},{"location":"trossen/#robot-operating-system-ros","text":"Repository: Interbotix ROS Core","title":"Robot Operating System (ROS)"},{"location":"trossen/#interbotix-research-robotics-open-standard-irros-layers","text":"1. Hardware Layer - Defines actuators and sensors in the robot - Includes actuator interface, controllers, drivers, and motors 2. Driver Layer - Contains ROS wrappers to obtain data from sensors and actuators - Repository resides in this layer 3. Control Layer - ROS packages for all Interbotix robot configurations - Define parameters for actuators & sensors and manage launch files 4. Application Support Layer - Makes working with the robot easier 5. Research Layer - User-supplied custom code","title":"Interbotix Research Robotics Open Standard (IRROS) Layers"},{"location":"trossen/#ros-functionality","text":"Links hardware with software using nodes . Each node can serve a purpose: data collection, reasoning, output. ROS drivers and wrappers allow working with high-level details while covering actuator specifics.","title":"ROS Functionality"},{"location":"trossen/#ros-versions","text":"","title":"ROS Versions"},{"location":"trossen/#ros-1-noetic","text":"Uses a master node to manage all other nodes Good for research; can run on multiple robots with custom networking or ROS namespaces Many supporting resources, but considered legacy","title":"ROS 1 (Noetic)"},{"location":"trossen/#ros-2-galactic-humble-rolling","text":"Uses DDS (Data Distribution Service) ; no central master node Suitable for real-time robotics and multi-robot communication More modern and actively updated Made by Jasmin Lin.","title":"ROS 2 (Galactic, Humble, Rolling)"}]}